{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06518653",
   "metadata": {},
   "source": [
    "# Gas Prices Analysis and Forecasting Project\n",
    "\n",
    "**Author: Mehdi FATHALLAH**\n",
    "\n",
    "## Project Overview\n",
    "In this project, we will conduct a comprehensive analysis of gas price data using Apache Spark. The objectives include:\n",
    "- Data acquisition from a designated GitHub repository, covering three years of gas prices.\n",
    "- Data processing to enable analysis, including merging files and restructuring date information.\n",
    "- Exploratory data analysis to filter gas types and compute indices like 'Price Index' and 'Week Index'.\n",
    "- Data visualization to observe trends over time.\n",
    "- Building and evaluating a predictive model for next-day gas price forecasting.\n",
    "- Documenting the process in a structured and detailed manner.\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f65742",
   "metadata": {},
   "source": [
    "### Repository Cloning Function Explanation\n",
    "\n",
    "The following Python function `clone_and_move` is designed to automate the process of cloning a GitHub repository and relocating it to a target directory on the local file system.\n",
    "\n",
    "**Functionality:**\n",
    "- It takes two parameters: the URL of the GitHub repository (`repo_url`) and the path to the target directory (`target_directory`).\n",
    "- The function uses the `git clone` command to clone the repository from GitHub.\n",
    "- It then extracts the name of the repository from the URL, strips the '.git' extension if present, and moves the cloned repository to the specified target directory using the `shutil.move` method.\n",
    "\n",
    "This function streamlines the workflow for setting up a local working environment with data from a remote repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a273931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def clone_and_move(repo_url, target_directory):\n",
    "    \"\"\"\n",
    "    Clones a GitHub repository and moves it to the specified target directory.\n",
    "\n",
    "    Args:\n",
    "    repo_url (str): The URL of the GitHub repository.\n",
    "    target_directory (str): The directory where the repository should be moved.\n",
    "    \"\"\"\n",
    "    # Clone the repository\n",
    "    subprocess.run([\"git\", \"clone\", repo_url])\n",
    "\n",
    "    # Extract the repository name from the URL\n",
    "    repo_name = repo_url.split('/')[-1]\n",
    "    if repo_name.endswith('.git'):\n",
    "        repo_name = repo_name[:-4]\n",
    "\n",
    "    # Move the repository\n",
    "    source_directory = os.path.join(os.getcwd(), repo_name)\n",
    "    shutil.move(source_directory, target_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b9db04",
   "metadata": {},
   "source": [
    "### Cloning and Moving the GitHub Repository\n",
    "\n",
    "The `clone_and_move` function is invoked here with the specific task of cloning the \"GasPrices\" repository from GitHub and moving it to a local directory named `spark_project/`.\n",
    "\n",
    "**Details of the Operation:**\n",
    "- `repo_url`: `\"https://github.com/rvm-courses/GasPrices.git\"` - This is the URL of the GitHub repository containing the gas price data.\n",
    "- `target_directory`: `\"spark_project/\"` - This indicates that the cloned repository will be moved to a folder named `spark_project` in the current working directory.\n",
    "\n",
    "By executing this line, we ensure that all necessary data for the project is locally available in a structured and organized manner within the `spark_project` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84b7a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "clone_and_move(\"https://github.com/rvm-courses/GasPrices.git\", \"/GasPrices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7724ef19",
   "metadata": {},
   "source": [
    "### Handling of Warnings in the Python Environment\n",
    "\n",
    "In this code snippet, the Python `warnings` library is utilized to suppress warnings that might arise during code execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e756878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc5ffb8",
   "metadata": {},
   "source": [
    "### Loading and Previewing Gas Stations and Services Data\n",
    "\n",
    "In this section, we load and inspect two key datasets for the project: Gas Stations data and Services data for the year 2022.\n",
    "\n",
    "**Process Overview:**\n",
    "1. **Import Pandas**: We start by importing the `pandas` library, which is essential for data manipulation and analysis in Python.\n",
    "2. **File Paths**: We define the paths to the gzipped CSV files for both stations and services data.\n",
    "3. **Data Loading**:\n",
    "   - We use `pandas.read_csv` to load the data. \n",
    "   - The `compression='gzip'` parameter indicates that the files are gzipped.\n",
    "   - We specify `sep='|'` as the delimiter, as the data in these files is pipe-separated.\n",
    "4. **Data Preview**: We display the first few rows of each DataFrame (`stations_data` and `services_data`) using the `head()` function to get an initial understanding of the data structure and contents.\n",
    "\n",
    "This step is crucial for ensuring data integrity and understanding the dataset's format before proceeding with further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a49abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to the files\n",
    "stations_file_path = 'GasPrices/Stations2022.csv.gz'\n",
    "services_file_path = 'GasPrices/Services2022.csv.gz'\n",
    "\n",
    "# Loading the gzipped CSV files using the pipe '|' as a delimiter\n",
    "stations_data = pd.read_csv(stations_file_path, compression='gzip', sep='|')\n",
    "services_data = pd.read_csv(services_file_path, compression='gzip', sep='|')\n",
    "\n",
    "# Displaying the first few rows of each dataframe\n",
    "print(\"Stations Data:\")\n",
    "print(stations_data.head())\n",
    "print(\"\\nServices Data:\")\n",
    "print(services_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a58dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c97f73",
   "metadata": {},
   "source": [
    "### Initializing Apache Spark Session for Gas Prices Analysis\n",
    "\n",
    "This segment of the code initializes a Spark session, a fundamental step for using Spark's functionality in data processing and analysis:\n",
    "\n",
    "- `from pyspark.sql import SparkSession`: This line imports the `SparkSession` module from PySpark, which is an entry point to programming Spark with the DataFrame API.\n",
    "- `SparkSession.builder`: This method is used to construct a SparkSession.\n",
    "- `.appName(\"GasPricesAnalysis\")`: Sets the name of the application to \"GasPricesAnalysis\". This name will be shown in the Spark cluster UI and is useful for identifying the application.\n",
    "- `.config(\"spark.driver.memory\", \"20g\")`: Configures the memory allocation for the Spark driver to 20 gigabytes. The driver is responsible for various tasks, including collecting data from executors and performing further transformations.\n",
    "- `.config(\"spark.executor.memory\", \"20g\")`: Sets the memory for each executor to 20 gigabytes. Executors run the tasks and return results to the driver.\n",
    "- `.getOrCreate()`: This method either retrieves an existing SparkSession or, if none exists, creates a new one based on the configuration set.\n",
    "\n",
    "Overall, this configuration is tailored to handle large datasets effectively, ensuring sufficient memory allocation for both driver and executors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4de3e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GasPricesAnalysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"20g\") \\\n",
    "    .config(\"spark.executor.memory\", \"20g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee5ef03",
   "metadata": {},
   "source": [
    "### Loading and Preparing Gas Price Data with Spark\n",
    "\n",
    "In this part of the project, we initialize a Spark session and define a function to load and prepare the gas price data for analysis.\n",
    "\n",
    "**Key Steps Involved:**\n",
    "\n",
    "1. **Import Spark Functions and Types**: Essential modules for handling date-time functions and defining the schema are imported.\n",
    "2. **Spark Session Initialization**: A new Spark session named \"GasPricesAnalysis\" is created.\n",
    "3. **Schema Definition**: \n",
    "   - A schema is defined to map to the structure of the CSV files. It includes fields for station ID, postal code, type, latitude, longitude, date, gas type ID, gas type name, and price.\n",
    "   - Latitude and longitude are treated as `DoubleType` and are assumed to be scaled by 100000.\n",
    "   - The date field is defined as `TimestampType` for ease of date-time operations.\n",
    "4. **Data Loading and Preparation Function**: \n",
    "   - `load_and_prepare_gas_data`: This function reads the CSV files, applies the schema, merges them, and does necessary data transformations.\n",
    "   - Latitude and longitude are adjusted, and the date field is split into year, month, and week for more granular analysis.\n",
    "   - The DataFrame is registered as a temporary SQL table for Spark SQL operations.\n",
    "\n",
    "By executing this code, we set up a robust data foundation for our gas price analysis and forecasting tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08ee47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, year, month, weekofyear\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"GasPricesAnalysis\").getOrCreate()\n",
    "\n",
    "# Define the schema according to the CSV file's content\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", StringType(), True),\n",
    "    StructField(\"PostalCode\", StringType(), True),  # Read as string to preserve leading zeros\n",
    "    StructField(\"Type\", StringType(), True),\n",
    "    StructField(\"Latitude\", DoubleType(), True),  # Assuming these need to be divided by 100000\n",
    "    StructField(\"Longitude\", DoubleType(), True),  # Assuming these need to be divided by 100000\n",
    "    StructField(\"Date\", TimestampType(), True),\n",
    "    StructField(\"GasTypeID\", StringType(), True),\n",
    "    StructField(\"GasTypeName\", StringType(), True),\n",
    "    StructField(\"Price\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Function to load and prepare the gas data\n",
    "def load_and_prepare_gas_data(file_paths):\n",
    "    # Read and merge gas files\n",
    "    df = None\n",
    "    for file_path in file_paths:\n",
    "        new_df = spark.read.csv(file_path, schema=schema, sep=';', header=False, timestampFormat=\"yyyy-MM-dd'T'HH:mm:ss\")\n",
    "        if df is None:\n",
    "            df = new_df\n",
    "        else:\n",
    "            df = df.union(new_df)\n",
    "    \n",
    "    # Prepare latitude & longitude\n",
    "    df = df.withColumn(\"Latitude\", df[\"Latitude\"] / 100000)\n",
    "    df = df.withColumn(\"Longitude\", df[\"Longitude\"] / 100000)\n",
    "    \n",
    "    # Split date into year, month, week of the year\n",
    "    df = df.withColumn('Year', year('Date'))\n",
    "    df = df.withColumn('Month', month('Date'))\n",
    "    df = df.withColumn('Week', weekofyear('Date'))\n",
    "\n",
    "    # Register the DataFrame as a table for Spark SQL\n",
    "    df.createOrReplaceTempView(\"gas_prices\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Replace with the paths to your files\n",
    "file_paths = [\n",
    "    'spark_project/GasPrices/Prix2020.csv.gz',\n",
    "    'spark_project/GasPrices/Prix2019.csv.gz',\n",
    "    'spark_project/GasPrices/Prix2018.csv.gz'\n",
    "]\n",
    "\n",
    "# Load and prepare the data\n",
    "gas_prices_df = load_and_prepare_gas_data(file_paths)\n",
    "\n",
    "# Show the prepared DataFrame\n",
    "gas_prices_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7280830",
   "metadata": {},
   "source": [
    "### Counting Rows in the Gas Prices DataFrame\n",
    "\n",
    "This code snippet calculates and prints the total number of rows in the `gas_prices_df` DataFrame, providing a quick overview of the dataset's size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5cfea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count = gas_prices_df.count()\n",
    "print(\"Number of rows:\", row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7ef295",
   "metadata": {},
   "source": [
    "### Data Transformation and Analysis in Spark for Gas Prices Project\n",
    "\n",
    "This code is an integral part of a school project focused on analyzing gas prices using Apache Spark. \n",
    "\n",
    "- **Spark Session Initialization**: Ensures a Spark session is active with the specific application name \"GasPricesAnalysis\".\n",
    "- **Data Type Conversion**: The 'Date' column in the dataset is converted to a timestamp format for accurate temporal analysis.\n",
    "- **Average Daily Price Calculation**: The average price for each type of gas is calculated for each day.\n",
    "- **Dataframe Join**: This average daily price is then joined back to the original dataframe to maintain data integrity.\n",
    "- **Price Index Calculation**: A new column, 'PriceIndex', is created to measure the relative change in price against the daily average.\n",
    "- **Week Index Calculation**: The dataset is further enhanced by calculating the 'WeekIndex', which represents the number of weeks since the first week in the dataset.\n",
    "\n",
    "This step is pivotal in preparing the data for in-depth temporal analysis and trend observation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cf4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, weekofyear, to_date\n",
    "\n",
    "# Initialize Spark session (if not already initialized)\n",
    "spark = SparkSession.builder.appName(\"GasPricesAnalysis\").getOrCreate()\n",
    "\n",
    "# Convert the 'Date' column to a timestamp type if it's not already\n",
    "gas_prices_df = gas_prices_df.withColumn(\"Timestamp\", to_date(col(\"Date\"), \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "\n",
    "# Calculate the average price for each gas type for each day\n",
    "avg_price_per_day = gas_prices_df.groupBy('GasTypeName', 'Timestamp').agg(avg('Price').alias('AvgPricePerDay'))\n",
    "\n",
    "# Join this average price back to the original dataframe\n",
    "gas_prices_df = gas_prices_df.join(avg_price_per_day, ['GasTypeName', 'Timestamp'])\n",
    "\n",
    "# Calculate the Price Index\n",
    "gas_prices_df = gas_prices_df.withColumn(\"PriceIndex\", 100 * ((col(\"Price\") - col(\"AvgPricePerDay\")) / col(\"AvgPricePerDay\")))\n",
    "\n",
    "# Calculate the number of weeks since the first week in the data set\n",
    "# Find the first week in the dataset\n",
    "first_week = gas_prices_df.select(weekofyear(col(\"Timestamp\")).alias(\"Week\")).orderBy(\"Week\").first().Week\n",
    "\n",
    "# Calculate Week Index\n",
    "gas_prices_df = gas_prices_df.withColumn(\"WeekIndex\", weekofyear(col(\"Timestamp\")) - first_week + 1)\n",
    "\n",
    "# Show the results\n",
    "gas_prices_df.select(\"GasTypeName\", \"Timestamp\", \"Price\", \"AvgPricePerDay\", \"PriceIndex\", \"WeekIndex\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c4c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e27b4f0",
   "metadata": {},
   "source": [
    "### Visualizing Average Gas Prices Per Week Using Seaborn\n",
    "\n",
    "This code segment is focused on visualizing the weekly evolution of average gas prices for different gas types in France:\n",
    "\n",
    "1. **Data Aggregation**: \n",
    "   - The code aggregates the data to calculate the average price per day for each gas type based on the 'WeekIndex'.\n",
    "   - The `groupBy` method is used to organize the data by week and gas type, and the `avg` function computes the average prices.\n",
    "\n",
    "2. **Data Collection and Conversion**: \n",
    "   - The aggregated results, which are smaller in size than the original DataFrame, are collected and converted into a Pandas DataFrame for ease of plotting.\n",
    "\n",
    "3. **Plotting with Seaborn and Matplotlib**:\n",
    "   - Seaborn and Matplotlib libraries are used for plotting.\n",
    "   - A line plot is created where each gas type's average price trend over weeks is visualized.\n",
    "   - Styling is set to \"darkgrid\" for better readability, and the plot size is adjusted for clarity.\n",
    "\n",
    "4. **Visualization Output**:\n",
    "   - The plot displays trends in average gas prices, segmented by gas type, over different weeks.\n",
    "   - This visualization aids in understanding the temporal dynamics of gas prices in France.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46201f31",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Perform the aggregation to get the average price per day per gas type\n",
    "avg_prices = gas_prices_df.groupBy('WeekIndex', 'GasTypeName').agg(F.avg('Price').alias('AvgPricePerDay'))\n",
    "\n",
    "# Collect the result which will be much smaller than the original DataFrame\n",
    "avg_prices_result = avg_prices.collect()\n",
    "\n",
    "# Convert the result to a Pandas DataFrame\n",
    "df = pd.DataFrame(avg_prices_result, columns=['WeekIndex', 'GasTypeName', 'AvgPricePerDay'])\n",
    "\n",
    "# Now plot using Seaborn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "# Create a lineplot for each type of gas\n",
    "plt.figure(figsize=(14, 7))  # Adjust the size as needed\n",
    "lineplot = sns.lineplot(data=df, x='WeekIndex', y='AvgPricePerDay', hue='GasTypeName', marker=\"o\")\n",
    "\n",
    "plt.title('Weekly Evolution of Average Gas Price Over France')\n",
    "plt.xlabel('Week Index')\n",
    "plt.ylabel('Average Price for Gas Type in France')\n",
    "plt.legend(title='Gas Type')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14b7d37",
   "metadata": {},
   "source": [
    "### Implementing Lag Function in Spark for Gas Price Analysis\n",
    "\n",
    "This code segment applies the lag function in Spark to the `gas_prices_df` DataFrame, a key step in analyzing price trends over time:\n",
    "\n",
    "- **Window Specification**: \n",
    "  - A window is defined using `Window.partitionBy('GasTypeName').orderBy('Date')`, which groups data by gas type and orders it chronologically.\n",
    "  \n",
    "- **Applying the Lag Function**: \n",
    "  - The `lag` function is used to create a new column, 'LagPrice', which contains the previous day's price for each gas type. This is crucial for calculating day-to-day price changes.\n",
    "\n",
    "- **Handling Null Values**: \n",
    "  - Any null values generated by the lag operation (likely in the first row of each partition) are dropped to maintain data integrity.\n",
    "\n",
    "This approach is instrumental in preparing the dataset for further analysis, such as identifying trends and anomalies in gas pricing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85e74c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Assuming 'Price' is the correct column name and 'gas_prices_df' is a Spark DataFrame\n",
    "windowSpec = Window.partitionBy('GasTypeName').orderBy('Date')\n",
    "\n",
    "# Use Spark's function to refer to the column\n",
    "gas_prices_df = gas_prices_df.withColumn('LagPrice', F.lag(gas_prices_df['Price']).over(windowSpec))\n",
    "\n",
    "# Drop rows with null values that may have been created by lag function\n",
    "gas_prices_df = gas_prices_df.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f62daa",
   "metadata": {},
   "source": [
    "### Building and Evaluating a Random Forest Regression Model in Spark\n",
    "\n",
    "This code cell is dedicated to creating and evaluating a Random Forest Regression model using Spark's MLlib for forecasting gas prices:\n",
    "\n",
    "- **Spark Session Initialization**: A Spark session is initialized with a specific application name.\n",
    "\n",
    "- **Feature Engineering with Lag Function**:\n",
    "  - The `lag` function is applied to the `gas_prices_df` DataFrame to include the previous day's price ('LagPrice') for each gas type.\n",
    "  - Null values resulting from this operation are removed to clean the data.\n",
    "\n",
    "- **Data Preparation**: \n",
    "  - A `VectorAssembler` is used to transform 'LagPrice' into a feature vector.\n",
    "  \n",
    "- **Data Splitting**: \n",
    "  - The dataset is split into training (80%) and testing (20%) sets.\n",
    "\n",
    "- **Model Training**:\n",
    "  - A `RandomForestRegressor` model is initialized and trained using the training data within a Pipeline.\n",
    "\n",
    "- **Model Prediction and Evaluation**:\n",
    "  - Predictions are made on the test data.\n",
    "  - The model's performance is evaluated using Root Mean Squared Error (RMSE) metric.\n",
    "\n",
    "- **Visualization**:\n",
    "  - A scatter plot is created to visualize the actual vs. predicted prices, aiding in assessing the model's accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698beddb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"gas_price_forecasting\").getOrCreate()\n",
    "\n",
    "# Replace 'df' with 'gas_prices_df' as 'df' seems to be a Pandas DataFrame\n",
    "# Replace 'GasType' with 'GasTypeName' as used previously\n",
    "windowSpec = Window.partitionBy('GasTypeName').orderBy('Date')\n",
    "gas_prices_df = gas_prices_df.withColumn('LagPrice', F.lag('Price').over(windowSpec))\n",
    "\n",
    "# Drop rows with null values that may have been created by lag function\n",
    "gas_prices_df = gas_prices_df.na.drop()\n",
    "\n",
    "# Data Preparation\n",
    "# Assemble features using the actual Spark DataFrame 'gas_prices_df', not 'df'\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"LagPrice\"], outputCol=\"features\")\n",
    "\n",
    "# Data Splitting\n",
    "(trainingData, testData) = gas_prices_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Model Building\n",
    "# Initialize the RandomForestRegressor\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Price\")\n",
    "\n",
    "# Chain assembler and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[vectorAssembler, rf])\n",
    "\n",
    "# Train model. This also runs the assembler.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"Price\", \"Date\", \"GasTypeName\").show(5)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"Price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "# Plotting\n",
    "# For plotting, we need to convert the Spark DataFrame to a Pandas DataFrame\n",
    "pandas_df = predictions.select(\"Price\", \"prediction\").toPandas()\n",
    "\n",
    "# Now use seaborn to plot\n",
    "sns.scatterplot(data=pandas_df, x=\"Price\", y=\"prediction\")\n",
    "plt.xlabel(\"Actual Price\")\n",
    "plt.ylabel(\"Predicted Price\")\n",
    "plt.title(\"Dispersion Plot: Actual vs Predicted Price\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070ac05",
   "metadata": {},
   "source": [
    "### Conclusion and Future Work\n",
    "\n",
    "This project successfully demonstrated how Apache Spark can be leveraged for analyzing and forecasting gas prices. By implementing data transformations, exploratory analysis, and machine learning, we gained insights into the trends and patterns of gas prices. The RandomForestRegressor model, while foundational, opens the door for more complex predictive models in the future.\n",
    "\n",
    "Future work could explore enhancing the model's accuracy through advanced feature engineering, hyperparameter tuning, and incorporating external economic indicators. There's also potential in deploying the model for real-time predictions, using live data feeds. This project establishes a solid foundation for more sophisticated analyses in energy sector forecasting.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
